{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification\n",
    "\n",
    "##### Group number: 127\n",
    "##### Student 1 SID: 550251668\n",
    "##### Student 2 SID: 540303144 \n",
    "##### Student 3 SID: 520325185\n",
    "##### Student 4 SID: 530419471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“Python 3.10.8”的单元格需要ipykernel包。\n",
      "\u001b[1;31m使用所需的包 <a href='command:jupyter.createPythonEnvAndSelectController'>创建 Python 环境</a>。\n",
      "\u001b[1;31m或使用命令“d:/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall”安装“ipykernel”"
     ]
    }
   ],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# libraries for KNN and Random Forest.\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# libraries for Decision Tree, Bagging\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# libraries for Ada Boost and Gradient Boosting\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# libraries for Logistic Regression and Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "data = pd.read_csv(\"rice-final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "# 1. Replace missing value symbols ('?') with np.nan\n",
    "data = data.replace('?', np.nan)\n",
    "\n",
    "# 2. Separate predictors (X) from the class label (y)\n",
    "X = data.iloc[:, :-1] \n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# 3. Ensure all feature columns are numeric\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 4. Handle missing values by imputing with the  column-wise mean (univariate imputation)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# 5. Normalize features using Min–Max scaling to map all values into the [0, 1] interval\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# 6. Encode categorical class labels into binary values: \"class1\" → 0, \"class2\" → 1. \n",
    "y = y.replace({\"class1\": 0, \"class2\": 1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "\n",
    "print_data(X_scaled, y.to_numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    model = LogisticRegression()\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    model = GaussianNB()\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold, scoring='accuracy')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    # Initialize Decision Tree with entropy criterion\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "    # Cross-validate and compute accuracy scores\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold, scoring=\"accuracy\", n_jobs=None)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    # Define base Decision Tree with entropy criterion\n",
    "    base_dt = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0)\n",
    "    # Bagging ensemble using the base Decision Tree\n",
    "    bag = BaggingClassifier(\n",
    "        estimator=base_dt,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        bootstrap=True,\n",
    "        random_state=0,\n",
    "        n_jobs=None\n",
    "    )\n",
    "    # Cross-validate and compute accuracy scores\n",
    "    scores = cross_val_score(bag, X, y, cv=cvKFold, scoring=\"accuracy\", n_jobs=None)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    # Define base Decision Tree\n",
    "    base_dt = DecisionTreeClassifier(criterion=\"entropy\", max_depth=int(max_depth), random_state=0)\n",
    "    \n",
    "    # Initialize AdaBoost with SAMME.R (default)\n",
    "    ada = AdaBoostClassifier(\n",
    "        estimator=base_dt,\n",
    "        n_estimators=int(n_estimators),\n",
    "        learning_rate=float(learning_rate),\n",
    "        algorithm=\"SAMME.R\",\n",
    "        random_state=0\n",
    "    )\n",
    "    try:\n",
    "        # Cross-validation with SAMME.R\n",
    "        scores = cross_val_score(ada, X, y, cv=cvKFold, scoring=\"accuracy\")\n",
    "    except ValueError:\n",
    "        # Fallback: use SAMME if SAMME.R not supported\n",
    "        ada.set_params(algorithm=\"SAMME\")\n",
    "        scores = cross_val_score(ada, X, y, cv=cvKFold, scoring=\"accuracy\")\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    # Initialize Gradient Boosting classifier\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=int(n_estimators),\n",
    "        learning_rate=float(learning_rate),\n",
    "        random_state=0\n",
    "    )\n",
    "    # Cross-validate and compute accuracy scores\n",
    "    scores = cross_val_score(gb, X, y, cv=cvKFold, scoring=\"accuracy\")\n",
    "\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Run Decision Tree\n",
    "dt_acc = dtClassifier(X_scaled, y.to_numpy())\n",
    "\n",
    "# Run Bagging (Decision Trees)\n",
    "bag_acc = bagDTClassifier(\n",
    "    X_scaled, y.to_numpy(),\n",
    "    n_estimators=bag_n_estimators,\n",
    "    max_samples=bag_max_samples,\n",
    "    max_depth=bag_max_depth\n",
    ")\n",
    "# Run Ada Boost (Decision Trees)\n",
    "ada_acc = adaDTClassifier(\n",
    "    X_scaled, y.to_numpy(),\n",
    "    n_estimators=ada_n_estimators,\n",
    "    learning_rate=ada_learning_rate,\n",
    "    max_depth=ada_bag_max_depth\n",
    ")\n",
    "# Run Gradient Boosting\n",
    "gb_acc = gbClassifier(\n",
    "    X_scaled, y.to_numpy(),\n",
    "    n_estimators=gb_n_estimators,\n",
    "    learning_rate=gb_learning_rate\n",
    ")\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: \")\n",
    "print(\"NB average cross-validation accuracy: \")\n",
    "print(\"DT average cross-validation accuracy: \",f\"{dt_acc:.4f}\")\n",
    "print(\"Bagging average cross-validation accuracy: \",f\"{bag_acc:.4f}\")\n",
    "print(\"AdaBoost average cross-validation accuracy: \",f\"{ada_acc:.4f}\")\n",
    "print(\"GB average cross-validation accuracy: \",f\"{gb_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "cvKFold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    # Split train and test data set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=0\n",
    "    )\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    \n",
    "    # parameters grid\n",
    "    param_grid = {\n",
    "        \"n_neighbors\": [1, 3, 5, 7],\n",
    "        \"p\": [1, 2]                 \n",
    "    }\n",
    "    # perform grid search\n",
    "    gs = GridSearchCV(\n",
    "        estimator=knn,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"accuracy\",\n",
    "        cv=cvKFold,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    best_k = int(gs.best_params_[\"n_neighbors\"])\n",
    "    best_p = int(gs.best_params_[\"p\"])\n",
    "    best_cv_acc = float(gs.best_score_)\n",
    "\n",
    "    y_pred = gs.best_estimator_.predict(X_test)\n",
    "    test_acc = float(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return best_k, best_p, best_cv_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    # split train test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=0\n",
    "    )\n",
    "\n",
    "    # define classifier\n",
    "    rf = RandomForestClassifier(\n",
    "        criterion=\"entropy\",      \n",
    "        max_features=\"sqrt\",    \n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # parameter grid\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [10, 30, 60, 100],\n",
    "        \"max_leaf_nodes\": [6, 12],\n",
    "    }\n",
    "\n",
    "    # perform grid search\n",
    "    gs = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"accuracy\",\n",
    "        cv=cvKFold,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    best_n_estimators = int(gs.best_params_[\"n_estimators\"])\n",
    "    best_max_leaf_nodes = int(gs.best_params_[\"max_leaf_nodes\"])\n",
    "    best_cv_acc = float(gs.best_score_)\n",
    "\n",
    "    y_pred = gs.best_estimator_.predict(X_test)\n",
    "    test_acc = float(accuracy_score(y_test, y_pred))\n",
    "    macro_f1 = float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    weighted_f1 = float(f1_score(y_test, y_pred, average=\"weighted\"))\n",
    "    \n",
    "    return best_n_estimators, best_max_leaf_nodes, best_cv_acc, test_acc, macro_f1, weighted_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "best_k, best_p, knn_cv_acc, knn_test_acc = bestKNNClassifier(X_scaled, y)\n",
    "print(f\"KNN best k: {best_k:d}\")\n",
    "print(f\"KNN best p: {best_p:d}\")\n",
    "print(f\"KNN cross-validation accuracy: {knn_cv_acc:.4f}\")\n",
    "print(f\"KNN test set accuracy: {knn_test_acc:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "best_n, best_leaf, rf_cv_acc, rf_test_acc, rf_macro_f1, rf_weighted_f1 = bestRFClassifier(X_scaled, y)\n",
    "print(f\"RF best n_estimators: {best_n:d}\")\n",
    "print(f\"RF best max_leaf_nodes: {best_leaf:d}\")\n",
    "print(f\"RF cross-validation accuracy: {rf_cv_acc:.4f}\")\n",
    "print(f\"RF test set accuracy: {rf_test_acc:.4f}\")\n",
    "print(f\"RF test set macro average F1: {rf_macro_f1:.4f}\")\n",
    "print(f\"RF test set weighted average F1: {rf_weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
